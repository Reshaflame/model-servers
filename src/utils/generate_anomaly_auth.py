import pandas as pd
import gzip
import os


def generate_anomaly_only_file(auth_path='data/auth.txt.gz', 
                                redteam_path='data/redteam.txt.gz',
                                output_path='data/anomaly_auth.txt.gz'):
    print("[Generator] ğŸš€ Loading redteam ground truth...")
    redteam_columns = ["time", "user", "src_comp", "dst_comp"]
    with gzip.open(redteam_path, 'rt') as f:
        red_df = pd.read_csv(f, names=redteam_columns)

    redteam_events = set(zip(red_df["time"], red_df["user"], red_df["src_comp"], red_df["dst_comp"]))
    print(f"[Generator] âœ… Loaded {len(redteam_events)} redteam events.")

    print("[Generator] ğŸ“¥ Reading auth data in chunks...")
    chunksize = 100_000
    filtered_rows = []

    auth_columns = ["time", "src_user", "src_comp", "dst_comp", 
                "auth_type", "logon_type", "auth_orientation", "success"]

    with gzip.open(auth_path, 'rt') as auth_file:
        for chunk in pd.read_csv(auth_file, names=auth_columns, chunksize=chunksize):
            original_len = len(chunk)
            matches = chunk.apply(
                lambda row: (row["time"], row["src_user"], row["src_comp"], row["dst_comp"]) in redteam_events,
                axis=1
            )
            filtered = chunk[matches]
            filtered_rows.append(filtered)
            print(f"   â””â”€ Chunk processed: {original_len} rows â†’ {len(filtered)} anomalies")

    anomaly_df = pd.concat(filtered_rows, ignore_index=True)
    print(f"[Generator] âœ… Total anomaly rows collected: {len(anomaly_df)}")

    print(f"[Generator] ğŸ’¾ Saving to {output_path}...")
    with gzip.open(output_path, 'wt') as out_file:
        anomaly_df.to_csv(out_file, index=False)

    print("[Generator] ğŸ Done!")


if __name__ == "__main__":
    generate_anomaly_only_file()
